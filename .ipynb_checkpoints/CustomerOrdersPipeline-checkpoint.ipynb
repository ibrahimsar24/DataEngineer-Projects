{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7800391c-2d19-430e-912e-83010acc748c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.5.tar.gz (317.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.2/317.2 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.5-py2.py3-none-any.whl size=317747921 sha256=b0d0bcf56d9e487016904722a0580548d03c557774f483122dbc45e47dc21c6b\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/8f/cb/c0/cc57eb1bf0f9dc87cdaf2b0dbac49e58a210ff68d21d6fc709\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbfc0c9-4ef5-4122-88c8-1241f7bc3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,explode,sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1f93f1-1ce7-40ee-807b-cd4c59da287f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/14 08:44:30 WARN Utils: Your hostname, codespaces-f8e69a resolves to a loopback address: 127.0.0.1; using 10.0.1.40 instead (on interface eth0)\n",
      "25/05/14 08:44:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/14 08:44:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"CustomerOrderPipeline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "965306de-5960-45ef-b2e9-ad6219cedc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----------------+--------+------+----------+\n",
      "|customer_id| name|            email|order_id|amount|order_date|\n",
      "+-----------+-----+-----------------+--------+------+----------+\n",
      "|        101|Rahul|rahul@example.com|    5001|  2500|2024-05-01|\n",
      "|        101|Rahul|rahul@example.com|    5002|  1800|2024-05-10|\n",
      "|        102|Meena|meena@example.com|    5003|  3200|2024-05-05|\n",
      "+-----------+-----+-----------------+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"multiline\",\"true\").json(\"customer_orders.json\")\n",
    "df = df.withColumn(\"order\",explode(\"orders\"))\n",
    "flat_df = df.select(\n",
    "    col(\"customer_id\"),\n",
    "    col(\"name\"),\n",
    "    col(\"email\"),\n",
    "    col(\"order.order_id\").alias(\"order_id\"),\n",
    "    col(\"order.amount\").alias(\"amount\"),\n",
    "    col(\"order.date\").alias(\"order_date\")\n",
    ")\n",
    "flat_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63597269-946f-4591-9da9-d3d784ac64f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = flat_df.select(\"customer_id\",\"name\",\"email\").distinct()\n",
    "orders = flat_df.select(\"order_id\",\"customer_id\",\"amount\",\"order_date\")\n",
    "\n",
    "orders.write.partitionBy(\"customer_id\").mode(\"overwrite\").csv(\"orders_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b7b7b04-5144-40f5-8439-cd29e9f5a193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|total_spent|\n",
      "+-----------+-----------+\n",
      "|        101|       4300|\n",
      "|        102|       3200|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/14 08:44:47 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "total_spent = orders.groupBy(\"customer_id\").agg(sum(\"amount\").alias(\"total_spent\"))\n",
    "total_spent.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
